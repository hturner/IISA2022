---
title: "Efficient Code"
author: <br>Dr Heather Turner<br>Research Software Engineering Fellow, University of Warwick, UK<br>
date: 30 December 2022
output:
  xaringan::moon_reader:
    chakra: libs/remark-latest.min.js
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: ["default", "extra.css"]
    lib_dir: libs
    keep_md: true
    nature:
      beforeInit: "macros.js"
      highlightStyle: googlecode
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
always_allow_html: yes      
---


```{r setup, include=FALSE}
options(digits = 4)
options(width = 67)
library(knitr)
library(kableExtra)
opts_chunk$set(echo = TRUE, dev = "png", dpi = 300,
               comment = "#", eval = TRUE, 
               fig.width = 5, fig.height = 5, 
               knitr.table.format = "markdown")
# trim white space top and right of plot
knit_hooks$set(small.mar = function(before, options, envir) {
    if (before) par(mar = c(4, 5, 2, 1))
})
# trim white space when par won't work
library(magick)
knit_hooks$set(crop = function(before, options, envir) {
    if (before || isTRUE((fig.num <- options$fig.num) == 0L))
        return()
    paths = fig_path(options$fig.ext, options, fig.num)
    for (f in paths) image_write(image_trim(image_read(f)), f)
})
# function to put ``` when compiling Rmd
ticks <- function() "```"
set.seed(1)
```

class: inverse middle

# Memory management

---

# Overview

Objects created in R are stored in memory. This has the advantage that
objects can be accessed faster, but R slows down as the memory fills up.
Creating objects also takes time. Therefore

 - re-use temporary variables. The allocated storage will be re-used if
the vector has the same length.
 - save results for re-use, e.g. index variables
 - don't save intermediate results unnecessarily -- compute on-the-fly
 - remove large objects when no longer needed (with `rm()`)
 
---

# Basic data structures

Try to use the simplest data structure for your purpose
 - matrices vs. data frames 
 - character or integer vectors vs. factors
 - logical or integer vectors vs. numeric vectors
 - unnamed objects vs. named objects
    
It is especially important to use low-level structures for computation
- you can create richer objects as a final step before returning to the user.

???
L for integer

---

# Big Data

Modern computers have enough RAM to work with millions of records 
using standard functions.

Some packages to work more efficiently with big data:

 - **data.table** faster operations on data frames; read/write 
large CSVs
 - **feather**, **fst** read/write (parts of) binary files
 - **dplyr** manipulation of data in databases.
 - **bigmemory**, **biganalytics** faster matrix operations,
generalized linear models, kmeans

Parallelisation can also help, see later.

???
N.B. sparse matrices not as efficient as you might expect for general programming, may need big data for it to be important.

---

# Growing Objects

Adding to an object in a loop

```{r adding}
res <- NULL
for (i in 1:5000) res <- c(res, 1)
``` 
   
may force a copy to be made at each iteration, with each copy stored until the
loop has completed. 
   
It is far better to create an object of the necessary size first  

```{r empty.object}
res <- numeric(5000)
for (i in seq_along(res)) res[i] <- 1
```

---

# Copy-on-Change

R usually copies an object to make changes to it.

`tracemem` can be used to trace copies of an object

```{r trace.growing}
z <- NULL
for (i in 1:3){ z <- c(z,1); print(tracemem(z)) }
```
```{r trace.growing2}
z <- numeric(2); print(tracemem(z))
```

```{r trace.growing3}
for (i in 1:2){z[i] <- i;print(tracemem(z))}
``` 

???

makes copy for each separate block of code
e.g. if run with above in one go interactively no copies
     if run in separate chunks 1 copy

---

# Benchmarking

There will usually be many ways to write code for a given task. To compare
alternatives, we can use benchmark the code.

If the code is more than a single expression, create wrappers for each alternative
```{r benchmark-alternatives, eval = TRUE}
grow <- function(n){
  res <- NULL
  for (i in 1:n) res <- c(res, 1)
  res
}
pre_specify <- function(n){
  res <- numeric(n)
  for (i in seq_along(res)) res[i] <- 1
  res
}
``` 
---

# `bench::mark`

Then run the two alternatives with `bench::mark`. This function 
 - Runs each expression at least once and at most enough times to take 0.5s
 - Makes sure the two expressions return the same result!  
 
```{r benchmark, eval = TRUE}
library(bench)
(bm <- bench::mark(grow(5000), pre_specify(5000)))
```

* `GC` is the garbage collector which tidies up objects are deleted
* `itr/sec` is how many times the expression could be run per second

---

# Plotting benchmarks

Distribution tends to be right-skewed - focus on the median!


```{r, fig.align = "center", fig.height = 3, out.width = "60%"}
plot(bm)
```

---

# Scaling

Benchmarking can be difficult as the best option can depend on the size of the data, e.g. memory allocation can overshadow run time for small objects.

When thinking about how our code scales to bigger, we need to consider what we mean by "big"
 - number of rows or number of columns?
 - number of observations or number of factor levels?
 
---

# `bench::press()`
 
`bench::press()` can compare a function over  grid of parameters

```{r}
bench::press(
  n = c(10, 100),
  k = c(10, 1),
  bench::mark(gl(n, k, length = 1000))
)
```

---

# Your Turn

Suppose we have a matrix of data and a two-level factor
```{r}
nr <- 10
nc <- 50
X <- matrix(rnorm(nr * nc, 10, 3), nrow = nr)
grp <- gl(2, 25)
```

Use `bench::mark()` to compare the following ways to find the coefficients of a linear model fitted to each row

```{r, eval = FALSE}
# one
res <- vector("list", nr)
for(i in seq_len(nr)){
  res[[i]] <- coef(lm(X[i,] ~ grp))
}
do.call("cbind", res)
# two
res2 <- coef(lm(t(X) ~ grp))
```

---

# Fast options for common operations

* Sorting
 - Use `sort(x, partial = 1:10)` to get the top 10
 - Use `sort(x, decreasing = TRUE)` vs `rev(sort(x))`
* Generating numeric vectors
 - `seq.int()` vs `seq()` or `seq_along(x)`, `seq_len(n)` for special cases
 - `rep.int()` vs `rep()` or `rep_len(n)`
* `which.min()` and `which.max()` vs e.g. `which(x == min(x))`
* `anyNA(x)` vs `any(is.na(x))`

???
"int" stands for internal!

---

# For Loops

For loops are an intuitive way to write code, but can be very inefficient. 

`for` is a function, `:` or `seq_along` is another
function, each use of `[` is a call to a function, ..., so a loop
involves many nested function calls.

Therefore avoid for loops where possible, otherwise make as lean as 
possible, by taking whatever can be outside the loop.

---

# Vectorization

Vectorization is operating on vectors (or vector-like objects) rather than
individual elements and is generally faster.

Many operations in R are vectorized, e.g.
```{r vectoizedOp}
x <- 1:3
y <- 3:1
x == y
x + y
``` 

---

# Vectorization and Matrices

Vectorization applies to matrices too, not only through matrix algebra

```{r matrixAlg}
M <- matrix(1:4, nrow = 2, ncol = 2)
M + M
```   

but also vectorized functions

```{r vectorizedM}
M <- M + .3
round(M)
``` 

---

# Recycling

Vectorized functions will recycle shorter vectors to create vectors of the
same length

```{r recycling}
1:4 + 0:1 + 2
``` 

This is particularly useful for single values

```{r recycle1}
1:5 > 1
``` 

and for generating regular patterns

```{r recyclePaste}
paste0(rep(1:3, each = 2), c("a", "b"))
``` 

---

# `ifelse`

`ifelse` is a vectorised version of `if`/`else`

```{r ifelse}
x <- c(5, 2, 9, 12)
ifelse(x > 6, 2 * x, 3 * x)
``` 

Recycling is also very useful here

```{r recycleIfElse}
x <- 1:10
ifelse(x %% 2 == 0, 5, 12)
``` 

However indexing is more efficient than `ifelse`

```{r indexing}
y <- rep.int(12, 10)
y[x %% 2 == 0] <- 5
y
```

---

# Logical operations

Logical operators such as `&` and `|` are vectorized, e.g.

```{r}
x <- c(1, 0.6, 1.2, 0.4, 0.5)
x > 0.4 & x < 0.8
```
If we only want to compare vectors of length 1 the operators `&&` and `||` are more efficient as they only compute the RHS if needed

```{r}
x[1] > 0.4 && x[1] < 0.8
```
Make sure the vectors are of length 1, otherwise only the first element is compared, without a warning

```{r}
x > 0.4 && x < 0.8
```


---

# `apply`
`apply` provides a way to apply a function to every row or column of a matrix
```{r apply}
M <- matrix(1:20, 2, 10)
apply(M, 1, quantile, 0.75)
apply(M, 2, mean)
``` 

---

# `lapply`

`lapply` applies a given function to each element of a list
```{r lapply}
l <- list()
l$x <- 1:3
l$y <- 4:6
lapply(l, mean)
``` 

---

# `sapply` and `vapply`

`sapply` acts similarly to `lapply`, but tries to simplify the result
```{r sapply}
sapply(l, mean)
``` 

It is better to use `vapply` in programming as it ensures the returned object is of the expected type (and is slightly faster)

```{r vapply}
vapply(l, mean, numeric(1))
``` 


---

# Matrix Methods 

Functions such as `apply` are very general. R provides fast
implementations for special cases
```{r matMeth}
colMeans(M)
rowMeans(M)
colSums(M)
rowSums(M)
```   

---

# More Matrix Methods
`rowsum` sums over rows within each level of a grouping variable
```{r rowsum}
M <- matrix(1:8, 4)
rowsum(M, rep(1:2, each = 2))
``` 
The packages `matrixStats` provides even more "matricized"
operations, including medians and standard deviations
```{r matrixStats}
library(matrixStats); colMedians(M)
```
```{r matrixStats2}
rowSds(M)
```  

---

class: inverse middle

# Parallelisation

---

# Parallelisation

Most functions in R run on a single core of your machine. The 
**parallel** package, part of the default R distribution, provides 
parallel versions of the `apply`-type functions.

Parallelisation is most straight-forward to implement for
*embarrassingly parallel* problems, such as applying a function to
elements of a list.

The first step is to make a cluster from the available cores, here 2
```{r makeCluster}
library(parallel)
clus <- makeCluster(2)
``` 
You can use `parallel::detectCores() - 1` on your own computer; `parallelly::availableCores() - 1` is safer to use in general.

---

# parApply on Multiple Cores

`parApply` is used exactly as `apply`, but has an additional
argument to specify the cluster on which to run the code
```{r parApply}
res <- parApply(clus, M, 1, quantile, 0.75)
```
There are also special versions for applying to rows (R) and cols (C)
```{r parRapply}
res <- parRapply(clus, M, quantile, 0.75)
res <- parCapply(clus, M, mean)
```   

---

# parLapply and parSapply

Similarly `parLapply` and `parSapply` mirror `lapply` and `sapply`
```{r parLapply}
parLapply(clus, l, mean)
```
```{r parLapply2}
parSapply(clus, l, mean)
``` 
Any number of R commands can be run after the cluster has been created. At the end, run
```{r stopClus}
stopCluster(clus)
``` 

---

# Your Turn

The **efficient** package contains a function to simulate a game of snakes and 
ladders, returning the number of rolls required to win.


Parallelise the following code:
```{r}
library(efficient)
N <- 10
nrolls <- sapply(seq_len(N), snakes_ladders)
```

---

# General Principles
Try to use vectorized functions where possible.

Otherwise use the `apply` family (and parellelise if necessary). Custom
functions will often be useful here to pass to `apply` etc.

Try to keep for loops for true iterative computations or tasks that are fast
in any case (optimizing code takes time!)

---

# References

Good references on optimizing R code

Wickham, H, _Advanced R_ (2nd edn), _Improving performance section_, https://adv-r.hadley.nz/perf-improve.html

Gillespie, C and Lovelace, R, _Efficient R programming_, https://csgillespie.github.io/efficientR/